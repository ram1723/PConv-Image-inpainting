{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inpainting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlGoGbXYCF-w"
      },
      "source": [
        "# Partial Convolutions for Image Inpainting using PyTorch\n",
        "\n",
        "This is a PyTorch implementation of \"*Image Inpainting for Irregular Holes Using Partial Convolutions*\", https://arxiv.org/abs/1804.07723 by Guilin Liu, Fitsum A. Reda, Kevin J. Shih, Ting-Chun Wang, Andrew Tao and Bryan Catanzaro from NVIDIA. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJfp6R-Slh69"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boBX3Fs3RBoU"
      },
      "source": [
        "!pip install oyaml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uMm0-owCE3a"
      },
      "source": [
        "from torchvision import transforms, utils\n",
        "from torchvision.utils import save_image\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn.functional as F\n",
        "from torchvision import models\n",
        "from random import randint\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "import oyaml as yaml\n",
        "import numpy as np\n",
        "import datetime\n",
        "import random\n",
        "import torch\n",
        "import cv2\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQjXH699FGUE"
      },
      "source": [
        "## Generating random masks\n",
        "\n",
        "In the paper a technique based on occlusion/dis-occlusion between two consecutive frames in videos for creating random irregular masks is used. We've instead just made a simple mask-generator function which uses OpenCV to draw some random irregular shapes which are used for masks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUarsG9_FI2I"
      },
      "source": [
        "class MaskGenerator(object):\n",
        "\n",
        "    def __init__(self, height, width, channels=3,\n",
        "                 filepath=None):\n",
        "        \"\"\"Function for generating masks\n",
        "        Arguments:\n",
        "            height {int} -- Mask height\n",
        "            width {width} -- Mask width\n",
        "        Keyword Arguments:\n",
        "            channels {int} -- Channels to output (default: {3})\n",
        "            filepath {[type]} -- Load masks from filepath. If None, generate masks with OpenCV (default: {None})\n",
        "        \"\"\"\n",
        "\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "        self.channels = channels\n",
        "        self.filepath = filepath\n",
        "\n",
        "        # If filepath supplied, load the list of masks within the directory\n",
        "        self.mask_files = []\n",
        "        if self.filepath:\n",
        "            filenames = [f for f in os.listdir(self.filepath)]\n",
        "            self.mask_files = [f for f in filenames\n",
        "                               if any(filetype in f.lower()\n",
        "                                      for filetype\n",
        "                                      in ['.jpeg', '.png', '.jpg'])]\n",
        "            print(\"Found {} masks in {}\".format(len(self.mask_files),\n",
        "                                                   self.filepath))\n",
        "\n",
        "    def _generate_mask(self):\n",
        "\n",
        "        img = np.zeros((self.height, self.width, self.channels), np.uint8)\n",
        "\n",
        "        # Set size scale\n",
        "        size = int((self.width + self.height) * 0.03)\n",
        "        if self.width < 64 or self.height < 64:\n",
        "            raise Exception(\"Width and Height of mask must be at least 64!\")\n",
        "\n",
        "        # Draw random lines\n",
        "        for _ in range(randint(1, 20)):\n",
        "            x1, x2 = randint(1, self.width), randint(1, self.width)\n",
        "            y1, y2 = randint(1, self.height), randint(1, self.height)\n",
        "            thickness = randint(3, size)\n",
        "            cv2.line(img, (x1, y1), (x2, y2), (1, 1, 1), thickness)\n",
        "\n",
        "        # Draw random circles\n",
        "        for _ in range(randint(1, 20)):\n",
        "            x1, y1 = randint(1, self.width), randint(1, self.height)\n",
        "            radius = randint(3, size)\n",
        "            cv2.circle(img, (x1, y1), radius, (1, 1, 1), -1)\n",
        "\n",
        "        # Draw random ellipses\n",
        "        for _ in range(randint(1, 20)):\n",
        "            x1, y1 = randint(1, self.width), randint(1, self.height)\n",
        "            s1, s2 = randint(1, self.width), randint(1, self.height)\n",
        "            a1, a2, a3 = randint(3, 180), randint(3, 180), randint(3, 180)\n",
        "            thickness = randint(3, size)\n",
        "            cv2.ellipse(img, (x1, y1), (s1, s2), a1, a2, a3,\n",
        "                        (1, 1, 1), thickness)\n",
        "\n",
        "        return 1 - img\n",
        "\n",
        "    def _load_mask(self, rotation=True, dilation=True, cropping=True):\n",
        "\n",
        "        # Read image\n",
        "        mask = cv2.imread(os.path.join(self.filepath, np.random.choice(\n",
        "                                                        self.mask_files,\n",
        "                                                        1,\n",
        "                                                        replace=False\n",
        "                                                        )[0]))\n",
        "\n",
        "        # Random rotation\n",
        "        if rotation:\n",
        "            rand = np.random.randint(-180, 180)\n",
        "            M = cv2.getRotationMatrix2D((mask.shape[1]/2, mask.shape[0]/2),\n",
        "                                        rand, 1.5)\n",
        "            mask = cv2.warpAffine(mask, M, (mask.shape[1], mask.shape[0]))\n",
        "\n",
        "        # Random dilation\n",
        "        if dilation:\n",
        "            rand = np.random.randint(5, 47)\n",
        "            kernel = np.ones((rand, rand), np.uint8)\n",
        "            mask = cv2.erode(mask, kernel, iterations=1)\n",
        "\n",
        "        # Random cropping\n",
        "        if cropping:\n",
        "            x = np.random.randint(0, mask.shape[1] - self.width)\n",
        "            y = np.random.randint(0, mask.shape[0] - self.height)\n",
        "            mask = mask[y:y+self.height, x:x+self.width]\n",
        "\n",
        "        return (mask > 1).astype(np.uint8)\n",
        "\n",
        "    def sample(self):\n",
        "        return self._generate_mask()\n",
        "\n",
        "def main():\n",
        "  NUM_MASK = 19000\n",
        "  \n",
        "  DIR_NAME = 'val_mask'\n",
        "  if os.path.exists(DIR_NAME):\n",
        "    pass\n",
        "  else:\n",
        "    os.mkdir(DIR_NAME)\n",
        "  \n",
        "  mask_generator = MaskGenerator(256, 256, channels=3,filepath=None)\n",
        "\n",
        "  for idx in range(NUM_MASK):\n",
        "      mask = mask_generator.sample() * 255\n",
        "      cv2.imwrite('{}/{}.png'.format(DIR_NAME, idx), mask)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YekjcZsYEjkt"
      },
      "source": [
        "## Loading the train and validation datasets along with the masks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZtm4geCEg22"
      },
      "source": [
        "class InitDataset(Dataset):\n",
        "    def __init__(self, data_root, img_transform, mask_transform, data='train'):\n",
        "        super(InitDataset, self).__init__()\n",
        "        self.img_transform = img_transform\n",
        "        self.mask_transform = mask_transform\n",
        "\n",
        "        if data == 'train':\n",
        "            self.paths = glob('{}/train/**/*.jpg'.format(data_root),\n",
        "                              recursive=True)\n",
        "            self.mask_paths = glob('{}/mask/*.png'.format(data_root))\n",
        "        else:\n",
        "            self.paths = glob('{}/val/*.jpg'.format(data_root, data))\n",
        "            self.mask_paths = glob('{}/val_mask/*.png'.format(data_root))\n",
        "\n",
        "        self.N_mask = len(self.mask_paths)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = self._load_img(self.paths[index])\n",
        "        img = self.img_transform(img.convert('RGB'))\n",
        "        mask = Image.open(self.mask_paths[random.randint(0, self.N_mask - 1)])\n",
        "        mask = self.mask_transform(mask.convert('RGB'))\n",
        "        return img * mask, mask, img\n",
        "\n",
        "    def _load_img(self, path):\n",
        "        \"\"\"\n",
        "        For dealing with the error of loading image which is occured by the loaded image has no data.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            img = Image.open(path)\n",
        "        except:\n",
        "            extension = path.split('.')[-1]\n",
        "            for i in range(10):\n",
        "                new_path = path.split('.')[0][:-1] + str(i) + '.' + extension\n",
        "                try:\n",
        "                    img = Image.open(new_path)\n",
        "                    break\n",
        "                except:\n",
        "                    continue\n",
        "        return img\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-INsPWqtHwDz"
      },
      "source": [
        "## Defining the model\n",
        "\n",
        "## Partial Convolution Layer\n",
        "The key element here is ofcourse the partial convolutional layer. Basically, given the convolutional filter **W** and the corresponding bias *b*, the following partial convolution is applied instead of a normal convolution:\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MathiasGruber/PConv-Keras/master/data/images/eq1.PNG' />\n",
        "\n",
        "where ⊙ is element-wise multiplication and **M** is a binary mask of 0s and 1s. Importantly, after each partial convolution, the mask is also updated, so that if the convolution was able to condition its output on at least one valid input, then the mask is removed at that location, i.e.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MathiasGruber/PConv-Keras/master/data/images/eq2.PNG' />\n",
        "\n",
        "The result of this is that with a sufficiently deep network, the mask will eventually be all ones (i.e. disappear)\n",
        "\n",
        "## UNet Architecture\n",
        "The architechture essentially it's based on a UNet-like structure, where all normal convolutional layers are replace with partial convolutional layers, such that in all cases the image is passed through the network alongside the mask.\n",
        "\n",
        "<img src='https://raw.githubusercontent.com/MathiasGruber/PConv-Keras/master/data/images/architecture.png' />"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq4NCIonJQNr"
      },
      "source": [
        "class PartialConvolution(nn.Conv2d):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size,\n",
        "                 stride=1, padding=0, dilation=1, groups=1, bias=True,\n",
        "                 padding_mode='zeros'):\n",
        "        super(PartialConvolution, self).__init__(in_channels, out_channels,\n",
        "                                            kernel_size, stride=stride,\n",
        "                                            padding=padding, dilation=dilation,\n",
        "                                            groups=groups, bias=bias,\n",
        "                                            padding_mode=padding_mode)\n",
        "        # kernel for updating mask\n",
        "        self.mask_kernel = torch.ones(self.out_channels, self.in_channels,\n",
        "                                      self.kernel_size[0], self.kernel_size[1])\n",
        "        # sum1 for renormalization\n",
        "        self.sum1 = self.mask_kernel.shape[1] * self.mask_kernel.shape[2] \\\n",
        "                                              * self.mask_kernel.shape[3]\n",
        "        # Define the updated mask\n",
        "        self.update_mask = None\n",
        "        # Define the mask ratio (sum(1) / sum(M))\n",
        "        self.mask_ratio = None\n",
        "        # Initialize the weights for image convolution\n",
        "        torch.nn.init.xavier_uniform_(self.weight)\n",
        "\n",
        "    def forward(self, img, mask):\n",
        "        with torch.no_grad():\n",
        "            if self.mask_kernel.type() != img.type():\n",
        "                self.mask_kernel = self.mask_kernel.to(img)\n",
        "            # Create the updated mask\n",
        "            # for calcurating mask ratio (sum(1) / sum(M))\n",
        "            self.update_mask = F.conv2d(mask, self.mask_kernel,\n",
        "                                        bias=None, stride=self.stride,\n",
        "                                        padding=self.padding,\n",
        "                                        dilation=self.dilation,\n",
        "                                        groups=1)\n",
        "            # calculate mask ratio (sum(1) / sum(M))\n",
        "            self.mask_ratio = self.sum1 / (self.update_mask + 1e-8)\n",
        "            self.update_mask = torch.clamp(self.update_mask, 0, 1)\n",
        "            self.mask_ratio = torch.mul(self.mask_ratio, self.update_mask)\n",
        "\n",
        "        # calcurate WT . (X * M)\n",
        "        conved = torch.mul(img, mask)\n",
        "        conved = F.conv2d(conved, self.weight, self.bias, self.stride,\n",
        "                          self.padding, self.dilation, self.groups)\n",
        "\n",
        "        if self.bias is not None:\n",
        "            # Maltuply WT . (X * M) and sum(1) / sum(M) and Add the bias\n",
        "            bias_view = self.bias.view(1, self.out_channels, 1, 1)\n",
        "            output = torch.mul(conved - bias_view, self.mask_ratio) + bias_view\n",
        "            # The masked part pixel is updated to 0\n",
        "            output = torch.mul(output, self.mask_ratio)\n",
        "        else:\n",
        "            # Multiply WT . (X * M) and sum(1) / sum(M)\n",
        "            output = torch.mul(conved, self.mask_ratio)\n",
        "\n",
        "        return output, self.update_mask\n",
        "\n",
        "\n",
        "class UpsampleData(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "\n",
        "    def forward(self, dec_feature, enc_feature, dec_mask, enc_mask):\n",
        "        out = self.upsample(dec_feature)\n",
        "        out = torch.cat([out, enc_feature], dim=1)\n",
        "        out_mask = self.upsample(dec_mask)\n",
        "        out_mask = torch.cat([out_mask, enc_mask], dim=1)\n",
        "        return out, out_mask\n",
        "\n",
        "\n",
        "class PConvLayer(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, sample='none-3', dec=False,\n",
        "                 bn=True, active='relu', conv_bias=False):\n",
        "        super().__init__()\n",
        "        # Define the partial conv layer\n",
        "        if sample == 'down-7':\n",
        "            params = {\"kernel_size\": 7, \"stride\": 2, \"padding\": 3}\n",
        "        elif sample == 'down-5':\n",
        "            params = {\"kernel_size\": 5, \"stride\": 2, \"padding\": 2}\n",
        "        elif sample == 'down-3':\n",
        "            params = {\"kernel_size\": 3, \"stride\": 2, \"padding\": 1}\n",
        "        else:\n",
        "            params = {\"kernel_size\": 3, \"stride\": 1, \"padding\": 1}\n",
        "        self.conv = PartialConvolution(in_ch, out_ch,\n",
        "                                  params[\"kernel_size\"],\n",
        "                                  params[\"stride\"],\n",
        "                                  params[\"padding\"],\n",
        "                                  bias=conv_bias)\n",
        "\n",
        "        \n",
        "        if dec:\n",
        "            self.upcat = UpsampleData()\n",
        "        if bn:\n",
        "            bn = nn.BatchNorm2d(out_ch)\n",
        "        if active == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif active == 'leaky':\n",
        "            self.activation = nn.LeakyReLU(negative_slope=0.2)\n",
        "\n",
        "    def forward(self, img, mask, enc_img=None, enc_mask=None):\n",
        "        if hasattr(self, 'upcat'):\n",
        "            out, update_mask = self.upcat(img, enc_img, mask, enc_mask)\n",
        "            out, update_mask = self.conv(out, update_mask)\n",
        "        else:\n",
        "            out, update_mask = self.conv(img, mask)\n",
        "        if hasattr(self, 'bn'):\n",
        "            out = self.bn(out)\n",
        "        if hasattr(self, 'activation'):\n",
        "            out = self.activation(out)\n",
        "        return out, update_mask\n",
        "\n",
        "\n",
        "class PConvUNet(nn.Module):\n",
        "    def __init__(self, finetune=False, in_ch=3, layer_size=6):\n",
        "        super().__init__()\n",
        "        self.freeze_enc_bn = True if finetune else False\n",
        "        self.layer_size = layer_size\n",
        "\n",
        "        self.enc_1 = PConvLayer(in_ch, 64, 'down-7', bn=False)\n",
        "        self.enc_2 = PConvLayer(64, 128, 'down-5')\n",
        "        self.enc_3 = PConvLayer(128, 256, 'down-5')\n",
        "        self.enc_4 = PConvLayer(256, 512, 'down-3')\n",
        "        self.enc_5 = PConvLayer(512, 512, 'down-3')\n",
        "        self.enc_6 = PConvLayer(512, 512, 'down-3')\n",
        "        self.enc_7 = PConvLayer(512, 512, 'down-3')\n",
        "        self.enc_8 = PConvLayer(512, 512, 'down-3')\n",
        "\n",
        "        self.dec_8 = PConvLayer(512 + 512, 512, dec=True, active='leaky')\n",
        "        self.dec_7 = PConvLayer(512 + 512, 512, dec=True, active='leaky')\n",
        "        self.dec_6 = PConvLayer(512 + 512, 512, dec=True, active='leaky')\n",
        "        self.dec_5 = PConvLayer(512 + 512, 512, dec=True, active='leaky')\n",
        "        self.dec_4 = PConvLayer(512 + 256, 256, dec=True, active='leaky')\n",
        "        self.dec_3 = PConvLayer(256 + 128, 128, dec=True, active='leaky')\n",
        "        self.dec_2 = PConvLayer(128 + 64,   64, dec=True, active='leaky')\n",
        "        self.dec_1 = PConvLayer(64 + 3,      3, dec=True, bn=False,\n",
        "                                active=None, conv_bias=True)\n",
        "\n",
        "    def forward(self, img, mask):\n",
        "        enc_f, enc_m = [img], [mask]\n",
        "        for layer_num in range(1, self.layer_size+1):\n",
        "            if layer_num == 1:\n",
        "                feature, update_mask = \\\n",
        "                    getattr(self, 'enc_{}'.format(layer_num))(img, mask)\n",
        "            else:\n",
        "                enc_f.append(feature)\n",
        "                enc_m.append(update_mask)\n",
        "                feature, update_mask = \\\n",
        "                    getattr(self, 'enc_{}'.format(layer_num))(feature,\n",
        "                                                              update_mask)\n",
        "\n",
        "        assert len(enc_f) == self.layer_size\n",
        "\n",
        "        for layer_num in reversed(range(1, self.layer_size+1)):\n",
        "            feature, update_mask = getattr(self, 'dec_{}'.format(layer_num))(\n",
        "                    feature, update_mask, enc_f.pop(), enc_m.pop())\n",
        "\n",
        "        return feature, mask\n",
        "\n",
        "    def train(self, mode=True):\n",
        "        super().train(mode)\n",
        "        if not self.freeze_enc_bn:\n",
        "            return \n",
        "        for name, module in self.named_modules():\n",
        "            if isinstance(module, nn.BatchNorm2d) and 'enc' in name:\n",
        "                module.eval()\n",
        "\n",
        "def main():\n",
        "  size = (1, 3, 512, 512)\n",
        "  img = torch.ones(size)\n",
        "  mask = torch.ones(size)\n",
        "  mask[:, :, 128:-128, :][:, :, :, 128:-128] = 0\n",
        "\n",
        "  conv = PartialConvolution(3, 3, 3, 1, 1)\n",
        "  criterion = nn.L1Loss()\n",
        "  img.requires_grad = True\n",
        "\n",
        "  output, out_mask = conv(img, mask)\n",
        "  loss = criterion(output, torch.randn(size))\n",
        "  loss.backward()\n",
        "\n",
        "  assert (torch.sum(torch.isnan(conv.weight.grad)).item() == 0)\n",
        "  assert (torch.sum(torch.isnan(conv.bias.grad)).item() == 0)\n",
        "\n",
        "  model = PConvUNet()\n",
        "  before = model.enc_5.conv.weight[0][0]\n",
        "  print(before)\n",
        "  output, out_mask = model(img, mask)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TOsTL0nKGwL"
      },
      "source": [
        "## Defining the loss function\n",
        "\n",
        "This technique uses quite an intense loss function. The highlights of it are:\n",
        "\n",
        "* Per-pixel losses both for maskes and un-masked regions\n",
        "* Perceptual loss based on ImageNet pre-trained VGG-16 (*pool1, pool2 and pool3 layers*)\n",
        "* Style loss on VGG-16 features both for predicted image and for computed image (non-hole pixel set to ground truth)\n",
        "* Total variation loss for a 1-pixel dilation of the hole region\n",
        "\n",
        "The weighting of all these loss terms are as follows:\n",
        "<img src='https://raw.githubusercontent.com/MathiasGruber/PConv-Keras/master/data/images/eq7.PNG' />\n",
        "\n",
        "### VGG16 model for feature extraction\n",
        "The authors of the paper used PyTorch to implement the model. The VGG16 model was chosen for feature extraction. The [VGG16 model in PyTorch](https://pytorch.org/docs/stable/torchvision/models.html) was trained with the following image pre-processing:\n",
        "1. Divide the image by 255,\n",
        "2. Subtract [0.485, 0.456, 0.406] from the RGB channels, respectively,\n",
        "3. Divide the RGB channels by [0.229, 0.224, 0.225], respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUCetfQRKvO8"
      },
      "source": [
        "class InpaintingLoss(nn.Module):\n",
        "    def __init__(self, extractor, tv_loss='mean'):\n",
        "        super(InpaintingLoss, self).__init__()\n",
        "        self.tv_loss = tv_loss\n",
        "        self.l1 = nn.L1Loss()\n",
        "        self.extractor = extractor\n",
        "\n",
        "    def forward(self, input, mask, output, gt):\n",
        "        comp = mask * input + (1 - mask) * output\n",
        "\n",
        "        tv_loss = total_variation_loss(comp, mask, self.tv_loss)\n",
        "        \n",
        "        hole_loss = self.l1((1-mask) * output, (1-mask) * gt)\n",
        "\n",
        "        # Valid Pixel Loss\n",
        "        valid_loss = self.l1(mask * output, mask * gt)\n",
        "\n",
        "        # Perceptual Loss and Style Loss\n",
        "        feats_out = self.extractor(output)\n",
        "        feats_comp = self.extractor(comp)\n",
        "        feats_gt = self.extractor(gt)\n",
        "        perc_loss = 0.0\n",
        "        style_loss = 0.0\n",
        "        # Calculate the L1Loss for each feature map\n",
        "        for i in range(3):\n",
        "            perc_loss += self.l1(feats_out[i], feats_gt[i])\n",
        "            perc_loss += self.l1(feats_comp[i], feats_gt[i])\n",
        "            style_loss += self.l1(gram_matrix(feats_out[i]),\n",
        "                                  gram_matrix(feats_gt[i]))\n",
        "            style_loss += self.l1(gram_matrix(feats_comp[i]),\n",
        "                                  gram_matrix(feats_gt[i]))\n",
        "\n",
        "        return {'valid': valid_loss,\n",
        "                'hole': hole_loss,\n",
        "                'perc': perc_loss,\n",
        "                'style': style_loss,\n",
        "                'tv': tv_loss}\n",
        "\n",
        "\n",
        "# The network of extracting the feature for perceptual and style loss\n",
        "class VGG16FeatureExtractor(nn.Module):\n",
        "    MEAN = [0.485, 0.456, 0.406]\n",
        "    STD = [0.229, 0.224, 0.225]\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg16 = models.vgg16(pretrained=True)\n",
        "        normalization = Normalization(self.MEAN, self.STD)\n",
        "        # Define the each feature exractor\n",
        "        self.enc_1 = nn.Sequential(normalization, *vgg16.features[:5])\n",
        "        self.enc_2 = nn.Sequential(*vgg16.features[5:10])\n",
        "        self.enc_3 = nn.Sequential(*vgg16.features[10:17])\n",
        "\n",
        "        # fix the encoder\n",
        "        for i in range(3):\n",
        "            for param in getattr(self, 'enc_{}'.format(i+1)).parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input):\n",
        "        feature_maps = [input]\n",
        "        for i in range(3):\n",
        "            feature_map = getattr(self, 'enc_{}'.format(i+1))(feature_maps[-1])\n",
        "            feature_maps.append(feature_map)\n",
        "        return feature_maps[1:]\n",
        "\n",
        "\n",
        "# Normalization Layer for VGG\n",
        "class Normalization(nn.Module):\n",
        "    def __init__(self, mean, std):\n",
        "        super(Normalization, self).__init__()\n",
        "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
        "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # normalize img\n",
        "        if self.mean.type() != input.type():\n",
        "            self.mean = self.mean.to(input)\n",
        "            self.std = self.std.to(input)\n",
        "        return (input - self.mean) / self.std\n",
        "\n",
        "\n",
        "# Gram Matrix of feature maps\n",
        "def gram_matrix(feat):\n",
        "    (b, ch, h, w) = feat.size()\n",
        "    feat = feat.view(b, ch, h * w)\n",
        "    feat_t = feat.transpose(1, 2)\n",
        "    gram = torch.bmm(feat, feat_t) / (ch * h * w)\n",
        "    return gram\n",
        "\n",
        "\n",
        "def dialation_holes(hole_mask):\n",
        "    b, ch, h, w = hole_mask.shape\n",
        "    dilation_conv = nn.Conv2d(ch, ch, 3, padding=1, bias=False).to(hole_mask)\n",
        "    torch.nn.init.constant_(dilation_conv.weight, 1.0)\n",
        "    with torch.no_grad():\n",
        "        output_mask = dilation_conv(hole_mask)\n",
        "    updated_holes = output_mask != 0\n",
        "    return updated_holes.float()\n",
        "\n",
        "\n",
        "def total_variation_loss(image, mask, method):\n",
        "    hole_mask = 1 - mask\n",
        "    dilated_holes = dialation_holes(hole_mask)\n",
        "    colomns_in_Pset = dilated_holes[:, :, :, 1:] * dilated_holes[:, :, :, :-1]\n",
        "    rows_in_Pset = dilated_holes[:, :, 1:, :] * dilated_holes[:, :, :-1:, :]\n",
        "    if method == 'sum':\n",
        "        loss = torch.sum(torch.abs(colomns_in_Pset*(\n",
        "                    image[:, :, :, 1:] - image[:, :, :, :-1]))) + \\\n",
        "            torch.sum(torch.abs(rows_in_Pset*(\n",
        "                    image[:, :, :1, :] - image[:, :, -1:, :])))\n",
        "    else:\n",
        "        loss = torch.mean(torch.abs(colomns_in_Pset*(\n",
        "                    image[:, :, :, 1:] - image[:, :, :, :-1]))) + \\\n",
        "            torch.mean(torch.abs(rows_in_Pset*(\n",
        "                    image[:, :, :1, :] - image[:, :, -1:, :])))\n",
        "    return loss\n",
        "\n",
        "def main():\n",
        "  vgg = VGG16FeatureExtractor()\n",
        "  criterion = InpaintingLoss(VGG16FeatureExtractor(), vgg)\n",
        "\n",
        "  img = torch.randn(1, 3, 500, 500)\n",
        "  mask = torch.ones((1, 1, 500, 500))\n",
        "  mask[:, :, 250:, :][:, :, :, 250:] = 0\n",
        "  input = img * mask\n",
        "  out = torch.randn(1, 3, 500, 500)\n",
        "  loss = criterion(input, mask, out, img)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RraSCy5SLzw9"
      },
      "source": [
        "## Evaluation of the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCZtgwkRL1Yy"
      },
      "source": [
        "def evaluate(model, dataset, device, filename):\n",
        "    print('Start the evaluation')\n",
        "    model.eval()\n",
        "    image, mask, gt = zip(*[dataset[i] for i in range(8)])\n",
        "    image = torch.stack(image)\n",
        "    mask = torch.stack(mask)\n",
        "    gt = torch.stack(gt)\n",
        "    with torch.no_grad():\n",
        "        output, _ = model(image.to(device), mask.to(device))\n",
        "    output = output.to(torch.device('cpu'))\n",
        "    output_comp = mask * image + (1 - mask) * output\n",
        "\n",
        "    grid = make_grid(torch.cat([image, mask, output, output_comp, gt], dim=0))\n",
        "    save_image(grid, filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NDipXPfL7kR"
      },
      "source": [
        "## Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDYEcknCL-N1"
      },
      "source": [
        "def create_ckpt_dir():\n",
        "    ckpt_dir = \"ckpt\"\n",
        "    if (os.path.exists(\"ckpt\")):\n",
        "      return ckpt_dir\n",
        "    else: \n",
        "      os.mkdir(ckpt_dir)\n",
        "      os.mkdir(os.path.join(ckpt_dir, \"val_vis\"))\n",
        "      os.mkdir(os.path.join(ckpt_dir, \"models\"))\n",
        "      return ckpt_dir\n",
        "\n",
        "\n",
        "def to_items(dic):\n",
        "    return dict(map(_to_item, dic.items()))\n",
        "\n",
        "\n",
        "def _to_item(item):\n",
        "    return item[0], item[1].item()\n",
        "\n",
        "\n",
        "class Config(dict):\n",
        "    def __init__(self, conf_file):\n",
        "        with open(conf_file, \"r\") as f:\n",
        "            config = yaml.safe_load(f)\n",
        "        self._conf = config\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        if self._conf.get(name) is None:\n",
        "            return None\n",
        "\n",
        "        return self._conf[name]\n",
        "\n",
        "\n",
        "def conf_to_param(config: dict) -> dict:\n",
        "    dind_keys = []\n",
        "    rm_keys = []\n",
        "    for key, val in config.items():\n",
        "        if isinstance(val, dict):\n",
        "            dind_keys.append(key)\n",
        "        elif not type(val) in [float, int, bool, str]:\n",
        "            rm_keys.pop(key)\n",
        "\n",
        "    for target in dind_keys:\n",
        "        val = config.pop(target)\n",
        "        config.update(val)\n",
        "    for target in rm_keys:\n",
        "        del config[target]\n",
        "\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_state_dict_on_cpu(obj):\n",
        "    cpu_device = torch.device(\"cpu\")\n",
        "    state_dict = obj.state_dict()\n",
        "    for key in state_dict.keys():\n",
        "        state_dict[key] = state_dict[key].to(cpu_device)\n",
        "    return state_dict\n",
        "\n",
        "\n",
        "def save_ckpt(ckpt_name, models, optimizers, n_iter):\n",
        "    ckpt_dict = {\"n_iter\": n_iter}\n",
        "    for prefix, model in models:\n",
        "        ckpt_dict[prefix] = get_state_dict_on_cpu(model)\n",
        "\n",
        "    for prefix, optimizer in optimizers:\n",
        "        ckpt_dict[prefix] = optimizer.state_dict()\n",
        "    torch.save(ckpt_dict, ckpt_name)\n",
        "\n",
        "\n",
        "def load_ckpt(ckpt_name, models, optimizers=None):\n",
        "    ckpt_dict = torch.load(ckpt_name)\n",
        "    for prefix, model in models:\n",
        "        assert isinstance(model, nn.Module)\n",
        "        model.load_state_dict(ckpt_dict[prefix], strict=False)\n",
        "    if optimizers is not None:\n",
        "        for prefix, optimizer in optimizers:\n",
        "            optimizer.load_state_dict(ckpt_dict[prefix])\n",
        "    return ckpt_dict[\"n_iter\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeTSnZS1NGzU"
      },
      "source": [
        "## The place where the training actually happens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e-xIIyXNGf7"
      },
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, step, config, device, model, dataset_train,\n",
        "                 dataset_val, criterion, optimizer):\n",
        "        self.stepped = step\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "        self.model = model\n",
        "        self.dataloader_train = DataLoader(dataset_train,\n",
        "                                           batch_size=config.batch_size,\n",
        "                                           shuffle=True)\n",
        "        self.dataset_val = dataset_val\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.evaluate = evaluate\n",
        "\n",
        "    def iterate(self):\n",
        "        print('Start the training')\n",
        "        for step, (input, mask, gt) in enumerate(self.dataloader_train):\n",
        "            loss_dict = self.train(step+self.stepped, input, mask, gt)\n",
        "            # report the loss\n",
        "            if step % self.config.log_interval == 0:\n",
        "                self.report(step+self.stepped, loss_dict)\n",
        "\n",
        "            # evaluation\n",
        "            if (step+self.stepped + 1) % self.config.vis_interval == 0 \\\n",
        "                    or step == 0 or step + self.stepped == 0:\n",
        "                \n",
        "                self.model.eval()\n",
        "                self.evaluate(self.model, self.dataset_val, self.device,\n",
        "                              '{}/val_vis/{}.png'.format(self.config.ckpt,\n",
        "                                                         step+self.stepped))\n",
        "\n",
        "            # save the model\n",
        "            if (step+self.stepped + 1) % self.config.save_model_interval == 0 \\\n",
        "                    or (step + 1) == self.config.max_iter:\n",
        "                print('Saving the model...')\n",
        "                save_ckpt('{}/models/{}.pth'.format(self.config.ckpt,\n",
        "                                                    step+self.stepped + 1),\n",
        "                          [('model', self.model)],\n",
        "                          [('optimizer', self.optimizer)],\n",
        "                          step+self.stepped + 1)\n",
        "\n",
        "            if step >= self.config.max_iter:\n",
        "                break\n",
        "\n",
        "    def train(self, step, input, mask, gt):\n",
        "        \n",
        "        self.model.train()\n",
        "\n",
        "        input = input.to(self.device)\n",
        "        mask = mask.to(self.device)\n",
        "        gt = gt.to(self.device)\n",
        "\n",
        "        # forward\n",
        "        output, _ = self.model(input, mask)\n",
        "        loss_dict = self.criterion(input, mask, output, gt)\n",
        "        loss = 0.0\n",
        "        for key, val in loss_dict.items():\n",
        "            coef = getattr(self.config, '{}_coef'.format(key))\n",
        "            loss += coef * val\n",
        "\n",
        "        # updates the model's params\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        loss_dict['total'] = loss\n",
        "        return to_items(loss_dict)\n",
        "\n",
        "    def report(self, step, loss_dict):\n",
        "        print('[STEP: {:>6}] | Valid Loss: {:.6f} | Hole Loss: {:.6f}'\\\n",
        "              '| TV Loss: {:.6f} | Perc Loss: {:.6f}'\\\n",
        "              '| Style Loss: {:.6f} | Total Loss: {:.6f}'.format(\n",
        "                        step, loss_dict['valid'], loss_dict['hole'],\n",
        "                        loss_dict['tv'], loss_dict['perc'],\n",
        "                        loss_dict['style'], loss_dict['total']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxYtsE96NPai"
      },
      "source": [
        "## Training Startpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHYEuIOTNSPm"
      },
      "source": [
        "config = Config(\"location of the config file\")\n",
        "config.ckpt = create_ckpt_dir()\n",
        "print(\"Check Point is '{}'\".format(config.ckpt))\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:{}\".format(config.cuda_id)\n",
        "                      if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "print(\"Loading the Model...\")\n",
        "model = PConvUNet(finetune=config.finetune,\n",
        "                  layer_size=config.layer_size)\n",
        "\n",
        "if config.finetune:\n",
        "    model.load_state_dict(torch.load(config.finetune)['model'])\n",
        "model.to(device)\n",
        "\n",
        "# Data Transformation\n",
        "img_tf = transforms.Compose([\n",
        "            transforms.ToTensor()\n",
        "            ])\n",
        "\n",
        "mask_tf = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(256),\n",
        "            transforms.ToTensor()\n",
        "            ])\n",
        "\n",
        "\n",
        "print(\"Loading the Validation Dataset...\")\n",
        "dataset_val = InitDataset(config.data_root,\n",
        "                      img_tf,\n",
        "                      mask_tf,\n",
        "                      data=\"val\")\n",
        "\n",
        "\n",
        "print(\"Loading the Training Dataset...\")\n",
        "dataset_train = InitDataset(config.data_root,\n",
        "                        img_tf,\n",
        "                        mask_tf,\n",
        "                        data=\"train\")\n",
        "\n",
        "# Loss fucntion\n",
        "criterion = InpaintingLoss(VGG16FeatureExtractor(),\n",
        "                            tv_loss=config.tv_loss).to(device)\n",
        "# Optimizer\n",
        "lr = config.finetune_lr if config.finetune else config.initial_lr\n",
        "if config.optim == \"Adam\":\n",
        "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad,\n",
        "                                        model.parameters()),\n",
        "                                    lr=lr,\n",
        "                                    weight_decay=config.weight_decay)\n",
        "elif config.optim == \"SGD\":\n",
        "    optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad,\n",
        "                                        model.parameters()),\n",
        "                                lr=lr,\n",
        "                                momentum=config.momentum,\n",
        "                                weight_decay=config.weight_decay)\n",
        "\n",
        "start_iter = 0\n",
        "trainer = Trainer(start_iter, config, device, model, dataset_train,\n",
        "                    dataset_val, criterion, optimizer)\n",
        "trainer.iterate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cfc01Ow4xEm6"
      },
      "source": [
        "## Resources\n",
        "\n",
        "- \"Image Inpainting for Irregular Holes Using Partial Convolutions\", https://arxiv.org/abs/1804.07723"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1P4Lq1v4xg-b"
      },
      "source": [
        "## Citation\n",
        "\n",
        "```\n",
        "@inproceedings{liu2018partialpadding,\n",
        "   author    = {Guilin Liu and Kevin J. Shih and Ting-Chun Wang and Fitsum A. Reda and Karan Sapra and Zhiding Yu and Andrew Tao and Bryan Catanzaro},\n",
        "   title     = {Partial Convolution based Padding},\n",
        "   booktitle = {arXiv preprint arXiv:1811.11718},   \n",
        "   year      = {2018},\n",
        "}\n",
        "@inproceedings{liu2018partialinpainting,\n",
        "   author    = {Guilin Liu and Fitsum A. Reda and Kevin J. Shih and Ting-Chun Wang and Andrew Tao and Bryan Catanzaro},\n",
        "   title     = {Image Inpainting for Irregular Holes Using Partial Convolutions},\n",
        "   booktitle = {The European Conference on Computer Vision (ECCV)},   \n",
        "   year      = {2018},\n",
        "}\n",
        "```"
      ]
    }
  ]
}